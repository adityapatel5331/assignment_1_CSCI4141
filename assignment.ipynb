{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview of Assignment 1\n",
        "\n",
        "This assignment involves implementing term-document retrieval techniques, including the term-document incidence matrix, inverted index, Jaccard similarity, and TF-IDF. The objective is to retrieve a set of recipes from a dataset based on a provided set of ingredients as a query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#   Enter your details below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aditya Patel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Banner ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "B00930387"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q1: Setting up the libraries and environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: nltk in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: whoosh in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (2.7.4)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: click in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Piyush\n",
            "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\Piyush\n",
            "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip install pandas nltk whoosh scikit-learn\n",
        "import nltk\n",
        "import re\n",
        "import tokenize\n",
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q2: Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Piyush\n",
            "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\Piyush\n",
            "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\Piyush\n",
            "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         title  \\\n",
            "49995         Caramel Frosting   \n",
            "49996  Barbecued Chicken Wings   \n",
            "49997               Pound Cake   \n",
            "49998                    Slush   \n",
            "49999      Granny Ebert'S Hash   \n",
            "\n",
            "                                              directions  \n",
            "49995  let butter melt add sugar brown add milk bring...  \n",
            "49996  mix sauc brown sugar onion water mix bowl set ...  \n",
            "49997  bake hour put toothpick fork middl done toothp...  \n",
            "49998  combin sugar boil water stir cool add banana r...  \n",
            "49999  cover meat water teaspoon salt teaspoon pepper...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Make sure to download necessary nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data/food_recipes.csv')\n",
        "\n",
        "# Function that combines cleaning, tokenizing, removing stopwords, stemming, and lemmatizing\n",
        "def preprocess_text(text):\n",
        "    # Normalize text\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z]', ' ', text)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Stem and lemmatize text\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens]\n",
        "\n",
        "    # Join words back into single string\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "# Apply the comprehensive preprocessing function\n",
        "data['directions'] = data['directions'].apply(preprocess_text)\n",
        "\n",
        "# Display the last 5 rows of the dataset to verify changes\n",
        "print(data.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q3 Term-Document Incidence Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "data = pd.read_csv('./data/food_recipes.csv')\n",
        "\n",
        "# Ensure there is no missing text data\n",
        "texts = data['recipeText'].fillna('')  # Replace NaN with empty string\n",
        "\n",
        "# Initialize CountVectorizer with binary=True to create an incidence matrix\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "# Fit and transform the data\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Convert the result to a DataFrame for better readability\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "incidence_matrix = pd.DataFrame(X.toarray(), columns=terms, index=data.index)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(incidence_matrix.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   00  000  01  05  10  100  101  103  104  105  ...  zip  ziploc  zipper  \\\n",
            "0   0    0   0   0   0    0    0    0    0    0  ...    0       0       0   \n",
            "1   0    0   0   0   0    0    0    0    0    0  ...    0       0       0   \n",
            "2   0    0   0   0   0    0    0    0    0    0  ...    0       0       0   \n",
            "3   0    0   0   0   0    0    0    0    0    0  ...    0       0       0   \n",
            "4   0    0   0   0   0    0    0    0    0    0  ...    0       0       0   \n",
            "\n",
            "   zippered  zita  ziti  zucchini  zucchinis  zuchini  zwieback  \n",
            "0         0     0     0         0          0        0         0  \n",
            "1         0     0     0         0          0        0         0  \n",
            "2         0     0     0         0          0        0         0  \n",
            "3         0     0     0         0          0        0         0  \n",
            "4         0     0     0         0          0        0         0  \n",
            "\n",
            "[5 rows x 7790 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "data = pd.read_csv('./data/food_recipes.csv')\n",
        "\n",
        "\n",
        "texts = data['directions'].fillna('')  \n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "incidence_matrix = pd.DataFrame(X.toarray(), columns=terms, index=data.index)\n",
        "\n",
        "print(incidence_matrix.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q4 Inverted Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def map_function(data):\n",
        "    intermediate = []\n",
        "    # Adjust the column name as per your data schema, e.g., 'directions' or 'recipeText'\n",
        "    for index, row in data.iterrows():\n",
        "        # Normalize the text: convert to lower case and split into words\n",
        "        terms = row['directions'].lower().split()\n",
        "        for term in terms:\n",
        "            intermediate.append((term, index))\n",
        "    return intermediate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reduce_function(mapped_data):\n",
        "    inverted_index = {}\n",
        "    for key, value in mapped_data:\n",
        "        if key in inverted_index:\n",
        "            inverted_index[key].add(value)\n",
        "        else:\n",
        "            inverted_index[key] = {value}\n",
        "    return inverted_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_inverted_index_map_reduce(data):\n",
        "    mapped_data = map_function(data)\n",
        "    return reduce_function(mapped_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         title  \\\n",
            "8192                Corn Cakes   \n",
            "8196          Gourmet Potatoes   \n",
            "24582             Spanish Rice   \n",
            "8         Nolan'S Pepper Steak   \n",
            "8200        Cornbread Dressing   \n",
            "...                        ...   \n",
            "32749    Chicken Tostada Salad   \n",
            "8182     Chicken And Rice Bake   \n",
            "32759    Orange Mushroom Salad   \n",
            "24572               Deer Chili   \n",
            "12287  Betty Jane'S Onion Soup   \n",
            "\n",
            "                                              directions  \n",
            "8192   Combine all ingredients., Pour 4 inch circles ...  \n",
            "8196   Melt margarine., Cook onions until soft., Mix ...  \n",
            "24582  Fry bacon in skillet until crisp; remove bacon...  \n",
            "8      Roll steak strips in flour., Brown in skillet....  \n",
            "8200   Heat oven to 425Â°., Grease and heat in oven a ...  \n",
            "...                                                  ...  \n",
            "32749  In small jar, mix vinegar, honey, cumin, salt ...  \n",
            "8182   In 3-quart oblong baking dish, combine soup, w...  \n",
            "32759  Toss lettuce, orange slices, mushrooms and oni...  \n",
            "24572  Mix meat and chopped onion and place in open D...  \n",
            "12287  Saute onions in butter until tender. Add dash ...  \n",
            "\n",
            "[1676 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def inverted_index_search(recipe_data, inverted_index, search_terms):\n",
        "    try:\n",
        "        sets_of_indices = [inverted_index[term] for term in search_terms if term in inverted_index]\n",
        "        \n",
        "        if not sets_of_indices:\n",
        "            print(\"No search terms found in any document.\")\n",
        "            return pd.DataFrame()\n",
        "        valid_indices = set.intersection(*sets_of_indices)\n",
        "        if not valid_indices:\n",
        "            print(\"No documents contain all search terms.\")\n",
        "            return pd.DataFrame()\n",
        "        return recipe_data.loc[list(valid_indices), ['title', 'directions']]\n",
        "    \n",
        "    except KeyError as e:\n",
        "        # This block now explicitly catches missing terms in the inverted index\n",
        "        print(f\"Warning: Search term '{str(e).strip('[]')}' not found in any document.\")\n",
        "        return pd.DataFrame()\n",
        "inverted_index = create_inverted_index_map_reduce(data[:49999])\n",
        "\n",
        "result = inverted_index_search(data, inverted_index, ['onions'])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q5 Inverted Index using Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents containing 'cream':\n",
            "                                                   title  \\\n",
            "1                                  Jewell Ball'S Chicken   \n",
            "3                                          Chicken Funny   \n",
            "32771                                      Lite Crab Dip   \n",
            "5                               Cheeseburger Potato Soup   \n",
            "6                                    Rhubarb Coffee Cake   \n",
            "...                                                  ...   \n",
            "32739                                           Ugly Dip   \n",
            "32747                                    Company Chicken   \n",
            "32755                                  Preacher'S Coming   \n",
            "32758  None Such Prize Cookies(Makes 48 Cookies, 3-In...   \n",
            "32766                                   Strawberry Salad   \n",
            "\n",
            "                                              directions  \n",
            "1      Place chipped beef on bottom of baking dish., ...  \n",
            "3      Boil and debone chicken., Put bite size pieces...  \n",
            "32771  Combine yogurt, mayonnaise, cream cheese and s...  \n",
            "5      Wash potatoes; prick several times with a fork...  \n",
            "6      Cream sugar and butter., Add egg and beat well...  \n",
            "...                                                  ...  \n",
            "32739  Fry sausage, then drain well., Put both packag...  \n",
            "32747  Melt margarine., With knife, spread sour cream...  \n",
            "32755  Put crushed wafers in ungreased 9 x 13 pan. Me...  \n",
            "32758  Sift together flour, salt and baking soda. Cre...  \n",
            "32766  Combine jello, water, pineapple and berries., ...  \n",
            "\n",
            "[7259 rows x 2 columns]\n",
            "Search latency: 0.0183 ms\n",
            "Average search latency for 'Cream': 0.0029 ms\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q6 Jaccard Similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search Results:\n",
            "                          title  jaccard_similarity\n",
            "31277      Mexican Corn Pudding            0.250000\n",
            "46733  Sweet And Sour Meat Loaf            0.200000\n",
            "2586            Blonde Brownies            0.200000\n",
            "2349        Ramen Cabbage Salad            0.200000\n",
            "49050             Lemonade Cake            0.166667\n",
            "Average Service Latency: 352.6231 ms\n",
            "Search Results:\n",
            "                          title  jaccard_similarity\n",
            "31277      Mexican Corn Pudding            0.250000\n",
            "46733  Sweet And Sour Meat Loaf            0.200000\n",
            "2586            Blonde Brownies            0.200000\n",
            "2349        Ramen Cabbage Salad            0.200000\n",
            "49050             Lemonade Cake            0.166667\n",
            "Average Service Latency: 362.4426 ms\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    if not union:\n",
        "        return 0\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "def jaccard_similarity_search(recipe_data, search_terms):\n",
        "  \n",
        "    search_terms_set = set(search_terms)\n",
        "    recipe_data['jaccard_similarity'] = recipe_data['directions'].apply(\n",
        "        lambda x: jaccard_similarity(set(x.split()), search_terms_set)\n",
        "    )\n",
        "    return recipe_data.sort_values(by='jaccard_similarity', ascending=False)[['title', 'jaccard_similarity']]\n",
        "\n",
        "def measure_average_latency(recipe_data, search_terms, num_trials=100):\n",
        "   \n",
        "    latencies = []\n",
        "    for _ in range(num_trials):\n",
        "        start_time = time.perf_counter()\n",
        "        jaccard_similarity_search(recipe_data, search_terms)\n",
        "        end_time = time.perf_counter()\n",
        "        latencies.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
        "    \n",
        "    average_latency = sum(latencies) / len(latencies)\n",
        "    return average_latency\n",
        "\n",
        "# Load data from the CSV file\n",
        "data = pd.read_csv('./data/food_recipes.csv')\n",
        "\n",
        "# Define search terms\n",
        "search_terms = ['mix', 'bake']\n",
        "\n",
        "# Perform a single search and print results\n",
        "results = jaccard_similarity_search(data, search_terms)\n",
        "print(\"Search Results:\")\n",
        "print(results.head())\n",
        "\n",
        "# Measure and print the average service latency\n",
        "average_latency = measure_average_latency(data, search_terms)\n",
        "print(f\"Average Service Latency: {average_latency:.4f} ms\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Another Answer \n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    \"\"\"\n",
        "    Calculate the Jaccard similarity between two sets.\n",
        "    :param set1: Set of elements.\n",
        "    :param set2: Set of elements.\n",
        "    :return: Jaccard similarity score.\n",
        "    \"\"\"\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    if not union:\n",
        "        return 0\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "def jaccard_similarity_search(recipe_data, search_terms):\n",
        "    \"\"\"\n",
        "    Perform a Jaccard similarity search on the recipe data using given search terms.\n",
        "    :param recipe_data: DataFrame containing recipe data.\n",
        "    :param search_terms: List of terms to search for.\n",
        "    :return: DataFrame with titles and Jaccard similarity scores.\n",
        "    \"\"\"\n",
        "    search_terms_set = set(search_terms)\n",
        "    recipe_data['jaccard_similarity'] = recipe_data['directions'].apply(\n",
        "        lambda x: jaccard_similarity(set(x.split()), search_terms_set)\n",
        "    )\n",
        "    return recipe_data.sort_values(by='jaccard_similarity', ascending=False)[['title', 'jaccard_similarity']]\n",
        "\n",
        "def measure_average_latency(recipe_data, search_terms, num_trials=100):\n",
        "    \"\"\"\n",
        "    Measure the average latency of performing the Jaccard similarity search.\n",
        "    :param recipe_data: DataFrame containing recipe data.\n",
        "    :param search_terms: List of terms to search for.\n",
        "    :param num_trials: Number of trials to average the latency over.\n",
        "    :return: Average latency in milliseconds.\n",
        "    \"\"\"\n",
        "    latencies = []\n",
        "    for _ in range(num_trials):\n",
        "        start_time = time.perf_counter()\n",
        "        jaccard_similarity_search(recipe_data, search_terms)\n",
        "        end_time = time.perf_counter()\n",
        "        latencies.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
        "    \n",
        "    average_latency = sum(latencies) / len(latencies)\n",
        "    return average_latency\n",
        "\n",
        "# Load data from the CSV file\n",
        "data = pd.read_csv('./data/food_recipes.csv')\n",
        "\n",
        "# Define search terms\n",
        "search_terms = ['mix', 'bake']\n",
        "\n",
        "# Perform a single search and print results\n",
        "results = jaccard_similarity_search(data, search_terms)\n",
        "print(\"Search Results:\")\n",
        "print(results.head())\n",
        "\n",
        "# Measure and print the average service latency\n",
        "average_latency = measure_average_latency(data, search_terms)\n",
        "print(f\"Average Service Latency: {average_latency:.4f} ms\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q7 TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Search Results:\n",
            "                 title  similarity\n",
            "49050    Lemonade Cake    0.623005\n",
            "11070   Corn Casserole    0.504928\n",
            "2586   Blonde Brownies    0.487715\n",
            "42277  Salisbury Steak    0.476386\n",
            "19690   Tuna Casserole    0.473113\n",
            "Average Service Latency: 1617.9188 ms\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "\n",
        "def perform_tfidf_search(data, query, top_n=5):\n",
        "    \"\"\"\n",
        "    Perform TF-IDF based search on the given dataset.\n",
        "    :param data: DataFrame containing the documents.\n",
        "    :param query: Search query as a string.\n",
        "    :param top_n: Number of top results to return.\n",
        "    :return: DataFrame with top_n results sorted by relevance.\n",
        "    \"\"\"\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(data['directions'])\n",
        "    query_tfidf = tfidf_vectorizer.transform([query])\n",
        "    \n",
        "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
        "    \n",
        "    results = data.loc[top_indices, :]\n",
        "    results['similarity'] = cosine_similarities[top_indices]\n",
        "    return results\n",
        "\n",
        "def measure_average_latency(data, query, num_trials=100):\n",
        "    \"\"\"\n",
        "    Measure the average service latency of the TF-IDF search.\n",
        "    :param data: DataFrame containing the documents.\n",
        "    :param query: Search query as a string.\n",
        "    :param num_trials: Number of trials to measure.\n",
        "    :return: Average latency in milliseconds.\n",
        "    \"\"\"\n",
        "    latencies = []\n",
        "    for _ in range(num_trials):\n",
        "        start_time = time.perf_counter()\n",
        "        perform_tfidf_search(data, query)\n",
        "        end_time = time.perf_counter()\n",
        "        latencies.append((end_time - start_time) * 1000)\n",
        "    \n",
        "    average_latency = sum(latencies) / len(latencies)\n",
        "    return average_latency\n",
        "\n",
        "# Load data from the CSV file\n",
        "data = pd.read_csv('./data/food_recipes.csv')\n",
        "\n",
        "# Define search query\n",
        "query = \"bake mix\"\n",
        "\n",
        "# Perform TF-IDF search and print results\n",
        "results = perform_tfidf_search(data, query)\n",
        "print(\"TF-IDF Search Results:\")\n",
        "print(results[['title', 'similarity']])\n",
        "\n",
        "# Measure and print average service latency\n",
        "average_latency = measure_average_latency(data, query)\n",
        "print(f\"Average Service Latency: {average_latency:.4f} ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q8 Search Index using Whoosh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/mnt/data/food_recipes.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load recipe data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m recipe_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/food_recipes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update with the correct path\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m recipe_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define the schema\u001b[39;00m\n\u001b[0;32m     14\u001b[0m schema \u001b[38;5;241m=\u001b[39m Schema(\n\u001b[0;32m     15\u001b[0m     title\u001b[38;5;241m=\u001b[39mTEXT(stored\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     16\u001b[0m     directions\u001b[38;5;241m=\u001b[39mTEXT(stored\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Piyush Patel\\OneDrive - Dalhousie University\\Desktop\\Summer 24\\CSCI4141\\assignment-1-adityapatel5331\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Piyush Patel\\OneDrive - Dalhousie University\\Desktop\\Summer 24\\CSCI4141\\assignment-1-adityapatel5331\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\Piyush Patel\\OneDrive - Dalhousie University\\Desktop\\Summer 24\\CSCI4141\\assignment-1-adityapatel5331\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Piyush Patel\\OneDrive - Dalhousie University\\Desktop\\Summer 24\\CSCI4141\\assignment-1-adityapatel5331\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\Piyush Patel\\OneDrive - Dalhousie University\\Desktop\\Summer 24\\CSCI4141\\assignment-1-adityapatel5331\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/food_recipes.csv'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from whoosh.index import create_in, open_dir\n",
        "from whoosh.fields import Schema, TEXT\n",
        "from whoosh.qparser import MultifieldParser\n",
        "from whoosh import scoring\n",
        "\n",
        "# Load recipe data\n",
        "recipe_data_path = '/mnt/data/food_recipes.csv'  # Update with the correct path\n",
        "recipe_data = pd.read_csv(recipe_data_path)\n",
        "\n",
        "# Define the schema\n",
        "schema = Schema(\n",
        "    title=TEXT(stored=True),\n",
        "    directions=TEXT(stored=True)\n",
        ")\n",
        "\n",
        "# Directory for the index\n",
        "index_dir = 'indexdir'\n",
        "if not os.path.exists(index_dir):\n",
        "    os.mkdir(index_dir)\n",
        "\n",
        "# Create or open the index\n",
        "if os.path.exists(os.path.join(index_dir, 'segments.gen')):\n",
        "    index = open_dir(index_dir)\n",
        "else:\n",
        "    index = create_in(index_dir, schema)\n",
        "\n",
        "# Index the recipe data\n",
        "start_time = time.time()\n",
        "writer = index.writer()\n",
        "for _, row in recipe_data.iterrows():\n",
        "    writer.add_document(title=row['title'], directions=row['directions'])\n",
        "writer.commit()  \n",
        "indexing_time = time.time() - start_time\n",
        "print(f\"Indexing completed in {indexing_time:.2f} seconds.\")\n",
        "\n",
        "# Function to list indexed documents for debugging\n",
        "def list_indexed_documents():\n",
        "    index = open_dir(index_dir)\n",
        "    with index.searcher() as searcher:\n",
        "        doc_count = searcher.doc_count()\n",
        "        print(f\"Total documents indexed: {doc_count}\")\n",
        "        for docnum in range(doc_count):\n",
        "            stored_fields = searcher.stored_fields(docnum)\n",
        "            print(stored_fields)\n",
        "\n",
        "# Function to perform a search in the Whoosh index\n",
        "def search_recipes(query_str):\n",
        "    index = open_dir(index_dir)\n",
        "    with index.searcher(weighting=scoring.TF_IDF()) as searcher:  # Using TF-IDF scoring\n",
        "        query_parser = MultifieldParser([\"title\", \"directions\"], schema=index.schema)\n",
        "        query = query_parser.parse(query_str)\n",
        "        \n",
        "        # Perform the search, limiting results to the top 5\n",
        "        results = searcher.search(query, limit=5)\n",
        "        return [(result['title'], result['directions'], result.score) for result in results]\n",
        "\n",
        "# Perform searches using existing queries\n",
        "queries = [\"curry\", \"chicken\", \"vegetarian\", \"dessert\", \"easy dinner\"]\n",
        "for query in queries:\n",
        "    results = search_recipes(query)\n",
        "    print(f\"Search results for '{query}':\")\n",
        "    for title, directions, score in results:\n",
        "        print(f\"Title: {title}, Directions: {directions}, Score: {score:.2f}\")\n",
        "    print()\n",
        "\n",
        "# Evaluate the performance of the Whoosh-based searching\n",
        "\n",
        "# Calculate the average time for indexing and searching operations over multiple runs\n",
        "total_search_time = 0\n",
        "for query in queries:\n",
        "    start_time = time.time()\n",
        "    results = search_recipes(query)\n",
        "    search_time = time.time() - start_time\n",
        "    total_search_time += search_time\n",
        "    print(f\"Search results for '{query}' in {search_time:.2f} seconds:\")\n",
        "\n",
        "average_search_time = total_search_time / len(queries)\n",
        "print(f\"Average search time: {average_search_time:.2f} seconds.\")\n",
        "\n",
        "# Calculate and analyze the space complexity of the index directory on disk\n",
        "total_size = 0\n",
        "for dirpath, dirnames, filenames in os.walk(index_dir):\n",
        "    for f in filenames:\n",
        "        fp = os.path.join(dirpath, f)\n",
        "        total_size += os.path.getsize(fp)\n",
        "\n",
        "print(f\"Total index directory size: {total_size / 1024:.2f} KB\")\n",
        "\n",
        "# Analysis of files in the index directory\n",
        "print(\"\\nIndex Directory Contents:\")\n",
        "for dirpath, dirnames, filenames in os.walk(index_dir):\n",
        "    for f in filenames:\n",
        "        fp = os.path.join(dirpath, f)\n",
        "        print(f\"{f} ({os.path.getsize(fp) / 1024:.2f} KB) - Purpose and usage detailed in documentation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q9 Performance Discussion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indexing completed in 42.81 seconds.\n",
            "Search results for 'curry' in 0.04 seconds:\n",
            "Search results for 'chicken' in 0.04 seconds:\n",
            "Search results for 'vegetarian' in 0.01 seconds:\n",
            "Search results for 'dessert' in 0.01 seconds:\n",
            "Search results for 'easy dinner' in 0.01 seconds:\n",
            "Average search time: 0.02 seconds.\n",
            "Total index directory size: 64951.10 KB\n",
            "\n",
            "Index Directory Contents:\n",
            "MAIN_3b7rll35o3ua08i4.seg (32474.71 KB) - Purpose and usage detailed in documentation.\n",
            "MAIN_7bxlaamck4wmq5f6.seg (32474.71 KB) - Purpose and usage detailed in documentation.\n",
            "MAIN_WRITELOCK (0.00 KB) - Purpose and usage detailed in documentation.\n",
            "_MAIN_1.toc (1.68 KB) - Purpose and usage detailed in documentation.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from whoosh.index import create_in, open_dir\n",
        "from whoosh.fields import Schema, TEXT\n",
        "from whoosh.qparser import MultifieldParser\n",
        "\n",
        "# Define the schema\n",
        "schema = Schema(\n",
        "    title=TEXT(stored=True),\n",
        "    directions=TEXT(stored=True)\n",
        ")\n",
        "\n",
        "# Directory for the index\n",
        "index_dir = 'indexdir'\n",
        "if not os.path.exists(index_dir):\n",
        "    os.mkdir(index_dir)\n",
        "\n",
        "# Create or open the index\n",
        "if os.path.exists(os.path.join(index_dir, 'segments.gen')):\n",
        "    index = open_dir(index_dir)\n",
        "else:\n",
        "    index = create_in(index_dir, schema)\n",
        "\n",
        "# Load your data\n",
        "recipe_data_path = './data/food_recipes.csv'\n",
        "recipe_data = pd.read_csv(recipe_data_path)\n",
        "\n",
        "# Measure indexing time\n",
        "start_time = time.time()\n",
        "writer = index.writer()\n",
        "for _, row in recipe_data.iterrows():\n",
        "    writer.add_document(title=row['title'], directions=row['directions'])\n",
        "writer.commit()  \n",
        "indexing_time = time.time() - start_time\n",
        "print(f\"Indexing completed in {indexing_time:.2f} seconds.\")\n",
        "\n",
        "# Function to perform a search and measure its time\n",
        "def search_recipes(query_str):\n",
        "    start_time = time.time()\n",
        "    index = open_dir(index_dir)\n",
        "    with index.searcher() as searcher:\n",
        "        query_parser = MultifieldParser([\"title\", \"directions\"], schema=index.schema)\n",
        "        query = query_parser.parse(query_str)\n",
        "        results = searcher.search(query, limit=5)\n",
        "        search_time = time.time() - start_time\n",
        "        return results, search_time\n",
        "\n",
        "# Search and calculate average search time\n",
        "queries = [\"curry\", \"chicken\", \"vegetarian\", \"dessert\", \"easy dinner\"]\n",
        "total_search_time = 0\n",
        "for query in queries:\n",
        "    results, search_time = search_recipes(query)\n",
        "    total_search_time += search_time\n",
        "    print(f\"Search results for '{query}' in {search_time:.2f} seconds:\")\n",
        "\n",
        "average_search_time = total_search_time / len(queries)\n",
        "print(f\"Average search time: {average_search_time:.2f} seconds.\")\n",
        "\n",
        "# Calculate the space complexity of the index directory\n",
        "total_size = 0\n",
        "for dirpath, dirnames, filenames in os.walk(index_dir):\n",
        "    for f in filenames:\n",
        "        fp = os.path.join(dirpath, f)\n",
        "        total_size += os.path.getsize(fp)\n",
        "\n",
        "print(f\"Total index directory size: {total_size / 1024:.2f} KB\")\n",
        "\n",
        "# Analysis of files in the index directory\n",
        "print(\"\\nIndex Directory Contents:\")\n",
        "for dirpath, dirnames, filenames in os.walk(index_dir):\n",
        "    for f in filenames:\n",
        "        fp = os.path.join(dirpath, f)\n",
        "        print(f\"{f} ({os.path.getsize(fp) / 1024:.2f} KB) - Purpose and usage detailed in documentation.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
