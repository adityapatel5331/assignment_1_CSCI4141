{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview of Assignment 1\n",
        "\n",
        "This assignment involves implementing term-document retrieval techniques, including the term-document incidence matrix, inverted index, Jaccard similarity, and TF-IDF. The objective is to retrieve a set of recipes from a dataset based on a provided set of ingredients as a query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#   Enter your details below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aditya Patel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Banner ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "B00930387"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q1: Setting up the libraries and environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: nltk in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: whoosh in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (2.7.4)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: click in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\piyush patel\\onedrive - dalhousie university\\desktop\\summer 24\\csci4141\\assignment-1-adityapatel5331\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Piyush\n",
            "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\Piyush\n",
            "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip install pandas nltk whoosh scikit-learn\n",
        "import nltk\n",
        "import re\n",
        "import tokenize\n",
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q2: Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Piyush\n",
            "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\Piyush\n",
            "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\Piyush\n",
            "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         title  \\\n",
            "49995         Caramel Frosting   \n",
            "49996  Barbecued Chicken Wings   \n",
            "49997               Pound Cake   \n",
            "49998                    Slush   \n",
            "49999      Granny Ebert'S Hash   \n",
            "\n",
            "                                              directions  \n",
            "49995  let butter melt add sugar brown add milk bring...  \n",
            "49996  mix sauc brown sugar onion water mix bowl set ...  \n",
            "49997  bake hour put toothpick fork middl done toothp...  \n",
            "49998  combin sugar boil water stir cool add banana r...  \n",
            "49999  cover meat water teaspoon salt teaspoon pepper...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Make sure to download necessary nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data/food_recipes.csv')\n",
        "\n",
        "# Function that combines cleaning, tokenizing, removing stopwords, stemming, and lemmatizing\n",
        "def preprocess_text(text):\n",
        "    # Normalize text\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z]', ' ', text)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Stem and lemmatize text\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens]\n",
        "\n",
        "    # Join words back into single string\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "# Apply the comprehensive preprocessing function\n",
        "data['directions'] = data['directions'].apply(preprocess_text)\n",
        "\n",
        "# Display the last 5 rows of the dataset to verify changes\n",
        "print(data.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q3 Term-Document Incidence Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "data = pd.read_csv('./data/food_recipes.csv')\n",
        "\n",
        "# Ensure there is no missing text data\n",
        "texts = data['recipeText'].fillna('')  # Replace NaN with empty string\n",
        "\n",
        "# Initialize CountVectorizer with binary=True to create an incidence matrix\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "# Fit and transform the data\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Convert the result to a DataFrame for better readability\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "incidence_matrix = pd.DataFrame(X.toarray(), columns=terms, index=data.index)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(incidence_matrix.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   00  000  01  05  10  100  101  103  104  105  ...  zip  ziploc  zipper  \\\n",
            "0   0    0   0   0   0    0    0    0    0    0  ...    0       0       0   \n",
            "1   0    0   0   0   0    0    0    0    0    0  ...    0       0       0   \n",
            "2   0    0   0   0   0    0    0    0    0    0  ...    0       0       0   \n",
            "3   0    0   0   0   0    0    0    0    0    0  ...    0       0       0   \n",
            "4   0    0   0   0   0    0    0    0    0    0  ...    0       0       0   \n",
            "\n",
            "   zippered  zita  ziti  zucchini  zucchinis  zuchini  zwieback  \n",
            "0         0     0     0         0          0        0         0  \n",
            "1         0     0     0         0          0        0         0  \n",
            "2         0     0     0         0          0        0         0  \n",
            "3         0     0     0         0          0        0         0  \n",
            "4         0     0     0         0          0        0         0  \n",
            "\n",
            "[5 rows x 7790 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "data = pd.read_csv('./data/food_recipes.csv')\n",
        "\n",
        "\n",
        "texts = data['directions'].fillna('')  \n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "incidence_matrix = pd.DataFrame(X.toarray(), columns=terms, index=data.index)\n",
        "\n",
        "print(incidence_matrix.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q4 Inverted Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def map_function(data):\n",
        "    intermediate = []\n",
        "    # Adjust the column name as per your data schema, e.g., 'directions' or 'recipeText'\n",
        "    for index, row in data.iterrows():\n",
        "        # Normalize the text: convert to lower case and split into words\n",
        "        terms = row['directions'].lower().split()\n",
        "        for term in terms:\n",
        "            intermediate.append((term, index))\n",
        "    return intermediate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reduce_function(mapped_data):\n",
        "    inverted_index = {}\n",
        "    for key, value in mapped_data:\n",
        "        if key in inverted_index:\n",
        "            inverted_index[key].add(value)\n",
        "        else:\n",
        "            inverted_index[key] = {value}\n",
        "    return inverted_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_inverted_index_map_reduce(data):\n",
        "    mapped_data = map_function(data)\n",
        "    return reduce_function(mapped_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         title  \\\n",
            "8192                Corn Cakes   \n",
            "8196          Gourmet Potatoes   \n",
            "24582             Spanish Rice   \n",
            "8         Nolan'S Pepper Steak   \n",
            "8200        Cornbread Dressing   \n",
            "...                        ...   \n",
            "32749    Chicken Tostada Salad   \n",
            "8182     Chicken And Rice Bake   \n",
            "32759    Orange Mushroom Salad   \n",
            "24572               Deer Chili   \n",
            "12287  Betty Jane'S Onion Soup   \n",
            "\n",
            "                                              directions  \n",
            "8192   Combine all ingredients., Pour 4 inch circles ...  \n",
            "8196   Melt margarine., Cook onions until soft., Mix ...  \n",
            "24582  Fry bacon in skillet until crisp; remove bacon...  \n",
            "8      Roll steak strips in flour., Brown in skillet....  \n",
            "8200   Heat oven to 425°., Grease and heat in oven a ...  \n",
            "...                                                  ...  \n",
            "32749  In small jar, mix vinegar, honey, cumin, salt ...  \n",
            "8182   In 3-quart oblong baking dish, combine soup, w...  \n",
            "32759  Toss lettuce, orange slices, mushrooms and oni...  \n",
            "24572  Mix meat and chopped onion and place in open D...  \n",
            "12287  Saute onions in butter until tender. Add dash ...  \n",
            "\n",
            "[1676 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def inverted_index_search(recipe_data, inverted_index, search_terms):\n",
        "    try:\n",
        "        sets_of_indices = [inverted_index[term] for term in search_terms if term in inverted_index]\n",
        "        \n",
        "        if not sets_of_indices:\n",
        "            print(\"No search terms found in any document.\")\n",
        "            return pd.DataFrame()\n",
        "        valid_indices = set.intersection(*sets_of_indices)\n",
        "        if not valid_indices:\n",
        "            print(\"No documents contain all search terms.\")\n",
        "            return pd.DataFrame()\n",
        "        return recipe_data.loc[list(valid_indices), ['title', 'directions']]\n",
        "    \n",
        "    except KeyError as e:\n",
        "        # This block now explicitly catches missing terms in the inverted index\n",
        "        print(f\"Warning: Search term '{str(e).strip('[]')}' not found in any document.\")\n",
        "        return pd.DataFrame()\n",
        "inverted_index = create_inverted_index_map_reduce(data[:49999])\n",
        "\n",
        "result = inverted_index_search(data, inverted_index, ['onions'])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q5 Inverted Index using Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents containing 'cream':\n",
            "                                                   title  \\\n",
            "1                                  Jewell Ball'S Chicken   \n",
            "3                                          Chicken Funny   \n",
            "32771                                      Lite Crab Dip   \n",
            "5                               Cheeseburger Potato Soup   \n",
            "6                                    Rhubarb Coffee Cake   \n",
            "...                                                  ...   \n",
            "32739                                           Ugly Dip   \n",
            "32747                                    Company Chicken   \n",
            "32755                                  Preacher'S Coming   \n",
            "32758  None Such Prize Cookies(Makes 48 Cookies, 3-In...   \n",
            "32766                                   Strawberry Salad   \n",
            "\n",
            "                                              directions  \n",
            "1      Place chipped beef on bottom of baking dish., ...  \n",
            "3      Boil and debone chicken., Put bite size pieces...  \n",
            "32771  Combine yogurt, mayonnaise, cream cheese and s...  \n",
            "5      Wash potatoes; prick several times with a fork...  \n",
            "6      Cream sugar and butter., Add egg and beat well...  \n",
            "...                                                  ...  \n",
            "32739  Fry sausage, then drain well., Put both packag...  \n",
            "32747  Melt margarine., With knife, spread sour cream...  \n",
            "32755  Put crushed wafers in ungreased 9 x 13 pan. Me...  \n",
            "32758  Sift together flour, salt and baking soda. Cre...  \n",
            "32766  Combine jello, water, pineapple and berries., ...  \n",
            "\n",
            "[7259 rows x 2 columns]\n",
            "Search latency: 0.0183 ms\n",
            "Average search latency for 'Cream': 0.0029 ms\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q6 Jaccard Similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search Results:\n",
            "                          title  jaccard_similarity\n",
            "31277      Mexican Corn Pudding            0.250000\n",
            "46733  Sweet And Sour Meat Loaf            0.200000\n",
            "2586            Blonde Brownies            0.200000\n",
            "2349        Ramen Cabbage Salad            0.200000\n",
            "49050             Lemonade Cake            0.166667\n",
            "Average Service Latency: 352.6231 ms\n",
            "Search Results:\n",
            "                          title  jaccard_similarity\n",
            "31277      Mexican Corn Pudding            0.250000\n",
            "46733  Sweet And Sour Meat Loaf            0.200000\n",
            "2586            Blonde Brownies            0.200000\n",
            "2349        Ramen Cabbage Salad            0.200000\n",
            "49050             Lemonade Cake            0.166667\n",
            "Average Service Latency: 362.4426 ms\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    if not union:\n",
        "        return 0\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "def jaccard_similarity_search(recipe_data, search_terms):\n",
        "  \n",
        "    search_terms_set = set(search_terms)\n",
        "    recipe_data['jaccard_similarity'] = recipe_data['directions'].apply(\n",
        "        lambda x: jaccard_similarity(set(x.split()), search_terms_set)\n",
        "    )\n",
        "    return recipe_data.sort_values(by='jaccard_similarity', ascending=False)[['title', 'jaccard_similarity']]\n",
        "\n",
        "def measure_average_latency(recipe_data, search_terms, num_trials=100):\n",
        "   \n",
        "    latencies = []\n",
        "    for _ in range(num_trials):\n",
        "        start_time = time.perf_counter()\n",
        "        jaccard_similarity_search(recipe_data, search_terms)\n",
        "        end_time = time.perf_counter()\n",
        "        latencies.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
        "    \n",
        "    average_latency = sum(latencies) / len(latencies)\n",
        "    return average_latency\n",
        "\n",
        "# Load data from the CSV file\n",
        "data = pd.read_csv('./data/food_recipes.csv')\n",
        "\n",
        "# Define search terms\n",
        "search_terms = ['mix', 'bake']\n",
        "\n",
        "# Perform a single search and print results\n",
        "results = jaccard_similarity_search(data, search_terms)\n",
        "print(\"Search Results:\")\n",
        "print(results.head())\n",
        "\n",
        "# Measure and print the average service latency\n",
        "average_latency = measure_average_latency(data, search_terms)\n",
        "print(f\"Average Service Latency: {average_latency:.4f} ms\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Another Answer \n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    \"\"\"\n",
        "    Calculate the Jaccard similarity between two sets.\n",
        "    :param set1: Set of elements.\n",
        "    :param set2: Set of elements.\n",
        "    :return: Jaccard similarity score.\n",
        "    \"\"\"\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    if not union:\n",
        "        return 0\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "def jaccard_similarity_search(recipe_data, search_terms):\n",
        "    \"\"\"\n",
        "    Perform a Jaccard similarity search on the recipe data using given search terms.\n",
        "    :param recipe_data: DataFrame containing recipe data.\n",
        "    :param search_terms: List of terms to search for.\n",
        "    :return: DataFrame with titles and Jaccard similarity scores.\n",
        "    \"\"\"\n",
        "    search_terms_set = set(search_terms)\n",
        "    recipe_data['jaccard_similarity'] = recipe_data['directions'].apply(\n",
        "        lambda x: jaccard_similarity(set(x.split()), search_terms_set)\n",
        "    )\n",
        "    return recipe_data.sort_values(by='jaccard_similarity', ascending=False)[['title', 'jaccard_similarity']]\n",
        "\n",
        "def measure_average_latency(recipe_data, search_terms, num_trials=100):\n",
        "    \"\"\"\n",
        "    Measure the average latency of performing the Jaccard similarity search.\n",
        "    :param recipe_data: DataFrame containing recipe data.\n",
        "    :param search_terms: List of terms to search for.\n",
        "    :param num_trials: Number of trials to average the latency over.\n",
        "    :return: Average latency in milliseconds.\n",
        "    \"\"\"\n",
        "    latencies = []\n",
        "    for _ in range(num_trials):\n",
        "        start_time = time.perf_counter()\n",
        "        jaccard_similarity_search(recipe_data, search_terms)\n",
        "        end_time = time.perf_counter()\n",
        "        latencies.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
        "    \n",
        "    average_latency = sum(latencies) / len(latencies)\n",
        "    return average_latency\n",
        "\n",
        "# Load data from the CSV file\n",
        "data = pd.read_csv('./data/food_recipes.csv')\n",
        "\n",
        "# Define search terms\n",
        "search_terms = ['mix', 'bake']\n",
        "\n",
        "# Perform a single search and print results\n",
        "results = jaccard_similarity_search(data, search_terms)\n",
        "print(\"Search Results:\")\n",
        "print(results.head())\n",
        "\n",
        "# Measure and print the average service latency\n",
        "average_latency = measure_average_latency(data, search_terms)\n",
        "print(f\"Average Service Latency: {average_latency:.4f} ms\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q7 TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Search Results:\n",
            "                 title  similarity\n",
            "49050    Lemonade Cake    0.623005\n",
            "11070   Corn Casserole    0.504928\n",
            "2586   Blonde Brownies    0.487715\n",
            "42277  Salisbury Steak    0.476386\n",
            "19690   Tuna Casserole    0.473113\n",
            "Average Service Latency: 1617.9188 ms\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "\n",
        "def perform_tfidf_search(data, query, top_n=5):\n",
        "    \"\"\"\n",
        "    Perform TF-IDF based search on the given dataset.\n",
        "    :param data: DataFrame containing the documents.\n",
        "    :param query: Search query as a string.\n",
        "    :param top_n: Number of top results to return.\n",
        "    :return: DataFrame with top_n results sorted by relevance.\n",
        "    \"\"\"\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(data['directions'])\n",
        "    query_tfidf = tfidf_vectorizer.transform([query])\n",
        "    \n",
        "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
        "    \n",
        "    results = data.loc[top_indices, :]\n",
        "    results['similarity'] = cosine_similarities[top_indices]\n",
        "    return results\n",
        "\n",
        "def measure_average_latency(data, query, num_trials=100):\n",
        "    \"\"\"\n",
        "    Measure the average service latency of the TF-IDF search.\n",
        "    :param data: DataFrame containing the documents.\n",
        "    :param query: Search query as a string.\n",
        "    :param num_trials: Number of trials to measure.\n",
        "    :return: Average latency in milliseconds.\n",
        "    \"\"\"\n",
        "    latencies = []\n",
        "    for _ in range(num_trials):\n",
        "        start_time = time.perf_counter()\n",
        "        perform_tfidf_search(data, query)\n",
        "        end_time = time.perf_counter()\n",
        "        latencies.append((end_time - start_time) * 1000)\n",
        "    \n",
        "    average_latency = sum(latencies) / len(latencies)\n",
        "    return average_latency\n",
        "\n",
        "# Load data from the CSV file\n",
        "data = pd.read_csv('./data/food_recipes.csv')\n",
        "\n",
        "# Define search query\n",
        "query = \"bake mix\"\n",
        "\n",
        "# Perform TF-IDF search and print results\n",
        "results = perform_tfidf_search(data, query)\n",
        "print(\"TF-IDF Search Results:\")\n",
        "print(results[['title', 'similarity']])\n",
        "\n",
        "# Measure and print average service latency\n",
        "average_latency = measure_average_latency(data, query)\n",
        "print(f\"Average Service Latency: {average_latency:.4f} ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q8 Search Index using Whoosh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indexing completed in 42.48 seconds.\n",
            "Search results for 'curry':\n",
            "Title: Curry Rice, Directions: In large pan, simmer pork chop cubes in salted water (enough to cover) 45 to 60 minutes., Add more hot water; enough to make as much curry as you want; then put in onions, carrots and potatoes. Salt and pepper to taste., Bring to boil, lower heat and simmer 2 hours, stirring occasionally, covered., Add the green pepper and simmer another 15 minutes., Add flour to water to make sauce for thickening., Stir into vegetables., Add curry powder., Serve over regular cooked rice, not instant., Leftovers freeze nicely., Score: 21.29\n",
            "Title: Chicken Curry, Directions: Rub chicken breasts with curry powder and put in baking dish. Pour on orange juice sweetened with a little honey., Bake until done in very slow oven (or adjust for microwave)., Thicken juices with cornstarch., Serve with brown or white rice., (Slow oven is 300° and would take about 1 1/2 hours, depending on the number of chicken breasts.), Score: 14.63\n",
            "Title: Curry Chicken, Directions: Mix milk, butter, mustard, honey, curry and salt in bowl until blended., Roll chicken in mixture, making sure it is well coated. Place chicken in roasting pan and place in 350° oven., Score: 14.63\n",
            "Title: Indian Chicken Curry, Directions: In a large saucepan, melt butter and saute onion, celery and apples for 5 minutes., Stir in curry and flour., Gradually stir in chicken broth, cream of coconut and half and half., Stir over moderate heat until sauce bubbles and thickens., Stir in chicken and season to taste with salt and pepper., Serve over rice., Score: 14.63\n",
            "Title: Bacon Curry Coleslaw, Directions: Combine raisins and lemon juice; heat gently until hot. Then set, aside until raisins are cool and plump., Meanwhile, finely shred the cabbage, dice onion, grate apple and cook and crumble bacon., Combine, apples,, raisins,, lemon, juice, and sugar;, add mayonnaise, and curry powder to apple mixture and blend., Just before, serving,, combine, cabbage, green onion, crumbled bacon, peanuts and mayonnaise., Score: 14.63\n",
            "\n",
            "Search results for 'chicken':\n",
            "Title: Amazing Chicken Pie, Directions: Boil chicken breasts for about 30 minutes. Put eggs in same pot with boiling chicken and boil for an additional 15 minutes. Drain water from chicken and eggs. Spray cool water on chicken and eggs to help cool faster. Spray rectangular casserole dish with Pam. Tear chicken breasts into small pieces and layer the bottom of rectangular casserole dish. Take hard-boiled eggs and remove shells. Press eggs with fork to make smaller pieces. Layer egg pieces on top of chicken and then season the top of chicken and eggs with salt and pepper. (You may also add a pinch of garlic.) Put cream of chicken soup (or chicken broth) on top of the layer of chicken and eggs and smooth with spoon. Mix flour, butter and milk together. Pour onto the top of chicken and eggs. Bake in oven at 375° for about 45 minutes or until the top is golden brown., Score: 39.97\n",
            "Title: Rosemary Chicken With Vegetables, Directions: Rinse chicken with water and pat dry., Salt and pepper entire chicken, then rub garlic onto chicken, then pat rosemary on chicken., Sprinkle juice of 1 lemon over chicken and stick the used lemon in chicken cavity., Put chicken in clay roasting oven or roasting bag., Surround chicken with chunks of potatoes, carrots and onion around chicken., Place in 375° oven and cook between 1 to 1 1/2 hours (check for doneness)., Serves 4., Score: 36.36\n",
            "Title: Lemon Chicken With Thyme, Directions: Mix flour, salt and pepper in bag., Rinse chicken and pat dry. Place in bag; shake to coat well., Brown chicken on 1 side in 1 tablespoon olive oil in skillet for 5 minutes; turn chicken., Add remaining olive oil., Cook for 5 minutes until brown., Remove chicken., Add margarine and onion to skillet., Saute for 3 minutes or until tender., Stir in broth, 2 tablespoons lemon juice and thyme., Bring to boil, stirring constantly., Add chicken; reduce heat to medium-low., Cook, covered for 7 minutes or until chicken is tender., Remove chicken to serving plate., Stir in remaining tablespoon of lemon juice in juices in skillet., Pour over chicken; sprinkle with parsley., Garnish with lemon wedges., Serve with rice., Serves 4., Score: 32.73\n",
            "Title: Beer Can Chicken, Directions: Remove and discard the neck, giblets and any excess fat from the chicken. Rinse chicken well inside and out with cold water. Lightly brush or spray chicken with vegetable oil, then season inside and out with the rub mixture. Open the can of beer and pour out half. Set the half full can of beer on a flat surface and slide the chicken over the top so the can fits inside the cavity. Transfer the bird to the grill, keeping the can upright. Grill over indirect medium heat 1 1/4 to 1 1/2 hours or until the chicken juices run clear and the internal temperature reaches 170° in the breast and 180° in the thickest part of the thigh. Wearing barbecue mitts, carefully remove the chicken and beer can from the grill. Be careful not to spill the beer since it will be hot. Let the chicken set for about 10 minutes before lifting it from the can. Discard beer. Cut chicken into serving pieces. Serve warm. Makes 4 to 6 servings., Score: 32.73\n",
            "Title: Chicken Breasts Lombardy, Directions: Place chicken between 2 sheets of wax paper; flatten to 1/8-inch thickness using a meat mallet or rolling pin. Dredge chicken lightly in flour. Place 4 pieces at a time in 2 tablespoons melted butter in a large skillet; cook over low heat 3 to 4 minutes on each side or until chicken is golden brown. Place chicken in a lightly greased 13 x 9 x 2-inch baking dish, overlapping edges. Repeat procedure with remaining chicken, adding 2 tablespoons butter. Reserve drippings. Saute mushrooms in remaining 1/4 cup butter; sprinkle evenly over chicken. Stir wine and chicken broth into pan drippings in skillet. Simmer 10 minutes, stirring occasionally. Spoon sauce evenly over chicken. Bake at 400° for 10 minutes. Combine cheeses and sprinkle over chicken. Bake an additional 5 minutes., Score: 32.70\n",
            "\n",
            "Search results for 'vegetarian':\n",
            "Title: Pan-Fried Cabbage, Directions: Pan-fried cabbage is simple and delicious., It can also be incorporated into a vegetarian appetizer or main dish strudel., Score: 11.13\n",
            "Title: Vegetarian Four-Bean Chili(Low-Calorie, Low-Fat, Low-Cholesterol)  , Directions: Heat oil in large pot over medium heat., Add onion, green pepper and garlic; saute 4 minutes or until onion is softened. Add carrot; cover and cook 2 minutes., Add zucchini, summer squash, chili powder, oregano and cumin., Saute 1 minute., Add tomatoes breaking up with wooden spoon the beer and salt., Bring to boiling. Lower heat; simmer, partially covered 15 minutes. Stir in all the beans., Simmer, stirring, until heated through., Score: 8.93\n",
            "Title: Vegetarian Chili With Rice, Directions: Combine kidney beans, Northern beans, undrained tomatoes, tomato sauce, pepper, onion, chili powder, sugar, basil, garlic and water in a large saucepan., Bring to boiling and reduce heat. Simmer, covered, for 15 minutes, stirring occasionally., Top each serving with 1/2 cup rice., Serves 4., Score: 8.93\n",
            "Title: Vegetarian Tortilla Soup, Directions: Heat oil over medium high heat in a large saucepan or Dutch oven., Add garlic and onion and cook until tender but not browned. Add tomatoes and cook for a few minutes more., Add the tofu cubes and stir gently until heated through, then add the corn, black beans, green chilies, vegetable broth, cilantro, cumin, salt, pepper and lime juice., Score: 8.93\n",
            "Title: Vegetarian Spaghetti Sauce, Directions: Crush garlic and chop onion and brown in oil., Add canned tomatoes and tomato sauce, mushrooms, salt, chopped squash (optional) and spices., Bring to boil; simmer for at least 2 hours., Good for weight watchers., Serve over spaghetti., Score: 8.93\n",
            "\n",
            "Search results for 'dessert':\n",
            "Title: Maria Solace'S Tiramisu Dessert, Directions: In a bowl, beat cream, confectioners sugar, coffee liqueur, vanilla extract and 2/3 of grated chocolate., Set aside remaining chocolate for top of dessert., In another bowl, beat whipping cream until peaks form., With rubber spatula, fold cream into Mascarpone mixture., In a third bowl, mix coffee, liqueur, half teaspoon vanilla extract., Dip Savoiardi biscuits into this mixture and line a cake pan (glass) or trifle bowl., Spread some of the cream mixture over the biscuits and continue layering until cream mixture is finished., Top with cocoa and grated chocolate., If you desire, you can pipe out rosettes with whipped cream on top of dessert and dust peaks with cocoa powder., Refrigerate., Score: 19.63\n",
            "Title: Strawberry Dessert(Excellent Dessert For Diabetics)  , Directions: Place a single layer of graham crackers in bottom of 13 x 9-inch pan., Mix pudding according to directions; add container of Cool Whip., Pour 1/3 of pudding mixture over graham crackers and cover all graham crackers with pudding., Add another layer of graham crackers., Pour next 1/3 of pudding., Add another layer of graham crackers., Pour remaining pudding on graham crackers. Clean and slice strawberries, reserving large ones to decorate top of dessert., Score: 18.35\n",
            "Title: Hot Fudge Sundae Cake, Directions: Heat, oven, to\t350°., In ungreased square pan (9 x 9 x 2-inch),, stir together flour, granulated sugar, 2 tablespoons cocoa,, baking, powder and salt., Mix in milk, oil and vanilla with, fork until smooth., Stir in nuts., Spread evenly in pan. Sprinkle, with, brown sugar and 1/4 cup cocoa., Pour hot water over, batter., Bake, 40 minutes., Let stand 15 minutes;, spoon into, dessert, dishes or cut into squares.\tInvert each square onto, dessert, plate., Top with ice cream and spoon sauce over each serving., Makes 9 servings., Score: 13.94\n",
            "Title: Strawberry Trifle, Directions: Mix cake and bake in 13 x 9 x 2-inch pan., Cool., Cut half of cake in cubes; reserve other half for another dessert., Dissolve Danish dessert in 3 1/2 cups cold water., Bring to boil 1 minute, stirring constantly until thick and clear., Cool., Add strawberries., Spread over cake cubes in pan., Mix pudding, milk and sour cream until smooth., Spread over strawberries., Top with Cool Whip., Garnish with almonds., Score: 13.94\n",
            "Title: Chocolate Eclair Dessert, Directions: Line a buttered 9 x 13-inch serving dish with half the graham crackers., Beat pudding mix and 3 1/2 cups milk in mixer bowl for 2 minutes., Fold in whipped topping., Layer half the pudding mixture, remaining graham crackers and remaining pudding mixture in prepared dish., Chill for 2 hours., Combine Choco-bake, corn syrup, vanilla and margarine in bowl; beat well., Add confectioners sugar and 3 tablespoons milk alternately, beating well after each addition., Spread over dessert., Score: 12.66\n",
            "\n",
            "Search results for 'easy dinner':\n",
            "Title: Joe C.'S Salad, Directions: Mix cottage cheese and Cool Whip with dry jello., Add drained fruit., Quick, easy and a refreshing taste for any lunch or dinner., Score: 14.12\n",
            "Title: Quick And Easy Casserole, Directions: Brown beef in skillet with salt, pepper and onion powder., Be sure to remove excess fat., Put cooked beef in a casserole dish. Next, layer sliced potatoes (peeled or not, thin to medium slices), then layer with veggie of your choice., Top with cream of whatever soup and about 1/2 cup water., Bake in conventional oven until potatoes are soft (about 1 hour) or microwave until potatoes are done (15 minutes)., Serve with dinner rolls., Score: 12.49\n",
            "Title: Easy Taco Dinner, Directions: Cook and stir ground beef and onion in 10-inch skillet until beef is light brown; drain., Stir in seasoning mix and water. Heat to boiling; reduce heat and simmer, uncovered, stirring every once in a while, for 10 minutes., Top with last ingredients. Serves 6., Score: 11.98\n",
            "Title: Easy Chicken Dinner, Directions: Combine ingredients in saucepan and heat thoroughly on low heat., Stir until heated., Serve over rice., Serves 4., Score: 11.98\n",
            "Title: Quick And Easy Skillet Dinner, Directions: Cook sausage in a 12-inch skillet over medium heat for 2 minutes., Add onion; cook, stirring occasionally, until tender (about 3 minutes)., Add rice; mix well., Stir in remaining ingredients except for green pepper., Bring to a boil, reduce heat, cover and simmer 20 minutes until liquid is absorbed., Stir in green pepper; let stand 5 minutes., Serve with additional picante sauce., Makes 6 to 8 servings., Score: 11.98\n",
            "\n",
            "Search results for 'curry' in 0.03 seconds:\n",
            "Search results for 'chicken' in 0.05 seconds:\n",
            "Search results for 'vegetarian' in 0.04 seconds:\n",
            "Search results for 'dessert' in 0.03 seconds:\n",
            "Search results for 'easy dinner' in 0.03 seconds:\n",
            "Average search time: 0.04 seconds.\n",
            "Total index directory size: 32476.39 KB\n",
            "\n",
            "Index Directory Contents:\n",
            "MAIN_10sabzk7qhiijbfy.seg (32474.71 KB) - Purpose and usage detailed in documentation.\n",
            "MAIN_WRITELOCK (0.00 KB) - Purpose and usage detailed in documentation.\n",
            "_MAIN_1.toc (1.68 KB) - Purpose and usage detailed in documentation.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from whoosh.index import create_in, open_dir\n",
        "from whoosh.fields import Schema, TEXT\n",
        "from whoosh.qparser import MultifieldParser\n",
        "from whoosh import scoring\n",
        "\n",
        "# Load recipe data\n",
        "recipe_data_path = './data/food_recipes.csv'\n",
        "recipe_data = pd.read_csv(recipe_data_path)\n",
        "\n",
        "# Define the schema\n",
        "schema = Schema(\n",
        "    title=TEXT(stored=True),\n",
        "    directions=TEXT(stored=True)\n",
        ")\n",
        "\n",
        "# Directory for the index\n",
        "index_dir = 'indexdir'\n",
        "if not os.path.exists(index_dir):\n",
        "    os.mkdir(index_dir)\n",
        "\n",
        "# Create or open the index\n",
        "if os.path.exists(os.path.join(index_dir, 'segments.gen')):\n",
        "    index = open_dir(index_dir)\n",
        "else:\n",
        "    index = create_in(index_dir, schema)\n",
        "\n",
        "# Index the recipe data\n",
        "start_time = time.time()\n",
        "writer = index.writer()\n",
        "for _, row in recipe_data.iterrows():\n",
        "    writer.add_document(title=row['title'], directions=row['directions'])\n",
        "writer.commit()  \n",
        "indexing_time = time.time() - start_time\n",
        "print(f\"Indexing completed in {indexing_time:.2f} seconds.\")\n",
        "\n",
        "# Function to list indexed documents for debugging\n",
        "def list_indexed_documents():\n",
        "    index = open_dir(index_dir)\n",
        "    with index.searcher() as searcher:\n",
        "        doc_count = searcher.doc_count()\n",
        "        print(f\"Total documents indexed: {doc_count}\")\n",
        "        for docnum in range(doc_count):\n",
        "            stored_fields = searcher.stored_fields(docnum)\n",
        "            print(stored_fields)\n",
        "\n",
        "# Function to perform a search in the Whoosh index\n",
        "def search_recipes(query_str):\n",
        "    index = open_dir(index_dir)\n",
        "    with index.searcher(weighting=scoring.TF_IDF()) as searcher:  # Using TF-IDF scoring\n",
        "        query_parser = MultifieldParser([\"title\", \"directions\"], schema=index.schema)\n",
        "        query = query_parser.parse(query_str)\n",
        "        \n",
        "        # Perform the search, limiting results to the top 5\n",
        "        results = searcher.search(query, limit=5)\n",
        "        return [(result['title'], result['directions'], result.score) for result in results]\n",
        "\n",
        "# Perform searches using existing queries\n",
        "queries = [\"curry\", \"chicken\", \"vegetarian\", \"dessert\", \"easy dinner\"]\n",
        "for query in queries:\n",
        "    results = search_recipes(query)\n",
        "    print(f\"Search results for '{query}':\")\n",
        "    for title, directions, score in results:\n",
        "        print(f\"Title: {title}, Directions: {directions}, Score: {score:.2f}\")\n",
        "    print()\n",
        "\n",
        "# Evaluate the performance of the Whoosh-based searching\n",
        "\n",
        "# Calculate the average time for indexing and searching operations over multiple runs\n",
        "total_search_time = 0\n",
        "for query in queries:\n",
        "    start_time = time.time()\n",
        "    results = search_recipes(query)\n",
        "    search_time = time.time() - start_time\n",
        "    total_search_time += search_time\n",
        "    print(f\"Search results for '{query}' in {search_time:.2f} seconds:\")\n",
        "\n",
        "average_search_time = total_search_time / len(queries)\n",
        "print(f\"Average search time: {average_search_time:.2f} seconds.\")\n",
        "\n",
        "# Calculate and analyze the space complexity of the index directory on disk\n",
        "total_size = 0\n",
        "for dirpath, dirnames, filenames in os.walk(index_dir):\n",
        "    for f in filenames:\n",
        "        fp = os.path.join(dirpath, f)\n",
        "        total_size += os.path.getsize(fp)\n",
        "\n",
        "print(f\"Total index directory size: {total_size / 1024:.2f} KB\")\n",
        "\n",
        "# Analysis of files in the index directory\n",
        "print(\"\\nIndex Directory Contents:\")\n",
        "for dirpath, dirnames, filenames in os.walk(index_dir):\n",
        "    for f in filenames:\n",
        "        fp = os.path.join(dirpath, f)\n",
        "        print(f\"{f} ({os.path.getsize(fp) / 1024:.2f} KB) - Purpose and usage detailed in documentation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q9 Performance Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(a) Compare the Service Latency of All the Search Methods\n",
        "Service Latencies:\n",
        "\n",
        "All methods (TF-IDF, BM25F, and Frequency) had an average search time of 0.04 seconds.\n",
        "\n",
        "Despite the different scoring techniques, all methods performed exceptionally well with very low and similar latencies. This means that Whoosh is highly efficient and quick regardless of the search method used.\n",
        "\n",
        "(b) Compare the Search Results and Explain Which One Performed Better and Why\n",
        "Search Results Analysis:\n",
        "\n",
        "TF-IDF: Balanced results considering term importance and frequency, providing highly relevant matches.\n",
        "BM25F: Expected to provide the most relevant results due to its refined scoring, though specific scores weren't given.\n",
        "Frequency: Fast and simple but less precise in determining relevance compared to the other methods."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
